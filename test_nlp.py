# -*- coding: utf-8 -*-
"""test_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qYr2pXim592HbE3WhYNht0T19noM1wCD
"""

!python3 -m nltk.downloader stopwords

import pandas as pd
import numpy as np
import re
import nltk
from nltk.stem.porter import PorterStemmer 
import os
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
nltk.download('punkt')
stop_words = set(stopwords.words('english')) 
from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten
from keras.layers import GlobalMaxPooling1D
from keras.layers.embeddings import Embedding
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.models import load_model

#from google.colab import drive
#drive.mount('/content/drive')

from keras.utils import get_file
import tarfile
data_dir = get_file('aclImdb_v1.tar.gz', 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', cache_subdir = "datasets",hash_algorithm = "auto", extract = True, archive_format = "auto")

my_tar = tarfile.open(data_dir)
my_tar.extractall('./data/') # specify which folder to extract to
my_tar.close()

"""**Pre-processing Steps**"""

data_folder="/test"
imdb_folder = "/aclimdb"
test_reviews = []
test_sentiments = []
ps = PorterStemmer()
for index,sentiment in enumerate(["/neg/", "/pos/"]):
    path = "./data"+ data_folder+imdb_folder + sentiment
    for filename in sorted(os.listdir(path)):
        with open(path + filename, 'r') as f:
            review = f.read() # Reading every text
            review = review.lower() # lower case text
            review = review.rstrip() # Remove the trailing characters
            review = review.replace("<br />", " ")
            review = re.sub(r"[^a-z ]", " ", review) 
            review = re.sub(r" +", " ", review)
            word_tokens = word_tokenize(review) 
            filter_sentence = [w for w in word_tokens if not w in stop_words] 
            filter_sentence = [] 
            for w in word_tokens: 
                if w not in stop_words: 
                    filter_sentence.append(w) 
            review=filter_sentence
            test_reviews.append(review)
            #print(word_tokens)
            #print(filtered_sentence)
            sentiment = [0, 0]
            sentiment[index] = 1
            test_sentiments.append(sentiment)

"""**Spliting Test data**"""

X_test=np.array(test_reviews)
print(X_test.shape)
y_test=np.array(test_sentiments)
print(y_test.shape)

"""**Vocabulary size in Test data**"""

all_words = []
for review in X_test:
    for word in review:
        all_words.append(word)
different_words = set(all_words)
vocab_length=len(different_words)+100
vocab_length = {x:i for i,x in enumerate(different_words)}
vocab_length=len(different_words)
print(len(different_words))

"""**One hot encoded words**"""

from keras.preprocessing.text import one_hot
X_test_encoded=[]
for review in X_test:
    encoded_review=[]
    result = [one_hot(word, vocab_length) for word in review]
    for x in result:
        for y in x:
            encoded_review.append(y)
    X_test_encoded.append(encoded_review)
encoded_review=np.array(encoded_review)
print(encoded_review.shape)
X_test_encoded=np.array(X_test_encoded)
print(X_test_encoded.shape)

"""**Pad sequence**"""

X_test_padded = pad_sequences(X_test_encoded, padding="post",maxlen=500)
print(X_test_padded[0].shape)
print(X_test_padded[1].shape)
print(X_test_padded.shape)
print(X_test_padded)

"""**Evaluation on Test data**"""

model = load_model('/models/20867979_NLP_model.h5')

scores = model.evaluate(X_test_padded, y_test, verbose=0)
print("Accuracy: %.2f%%" % (scores[1]*100))
print("Loss: %.2f%%" % (scores[0]*100))



"""**Accuracy and Loss plot**"""

import matplotlib.pyplot as plt
plt.plot(model.history['loss'])
plt.plot(model.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['Train','Validation'],loc='upper left')
plt.show()