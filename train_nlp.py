# -*- coding: utf-8 -*-
"""train_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TwqD34USDvU8nFR5l1_sMXFLg1acmHNA
"""

!python3 -m nltk.downloader stopwords

#from google.colab import drive
#drive.mount('/content/drive')

import pandas as pd
import numpy as np
import re
import nltk
from nltk.stem.porter import PorterStemmer 
import os
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
nltk.download('punkt')
stop_words = set(stopwords.words('english')) 
from numpy import array
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten
from keras.layers import GlobalMaxPooling1D
from keras.layers.embeddings import Embedding
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer

from keras.utils import get_file
import tarfile
data_dir = get_file('aclImdb_v1.tar.gz', 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', cache_subdir = "datasets",hash_algorithm = "auto", extract = True, archive_format = "auto")

my_tar = tarfile.open(data_dir)
my_tar.extractall('./data/') # specify which folder to extract to
my_tar.close()

"""The raw text is messy for these reviews so before we can do any analytics we need to do some pre-processing to clean up the data

**Pre-processing Steps**
"""

# reading data for train set
data_folder="/train"
imdb_folder = "/aclimdb"
train_reviews = []
train_sentiments = []
ps = PorterStemmer()
for index,sentiment in enumerate(["/neg/", "/pos/"]):
    path = "./data"+ imdb_folder + data_folder + sentiment
    for filename in sorted(os.listdir(path)):
        with open(path + filename, 'r') as f:
          # using regex 
            review = f.read() # Reading every text
            review = review.lower() # lower case text
            review = review.rstrip() # Remove the trailing characters
            review = review.replace("<br />", " ") # removing HTML words and replace with ,and white space
            review = re.sub(r"[^a-z ]", " ", review) 
            review = re.sub(r" +", " ", review)

            # print(review)
            #takes a sentence and the stopwords as inputs and returns the sentence without any stopwords
            word_tokens = word_tokenize(review) 
            filter_sentence = [w for w in word_tokens if not w in stop_words] 
            filter_sentence = [] 
            for w in word_tokens: 
                if w not in stop_words: 
                    filter_sentence.append(w) 
            review=filter_sentence
            train_reviews.append(review)
            #print(word_tokens)
            #print(filtered_sentence)
            sentiment = [0, 0]
            sentiment[index] = 1
            train_sentiments.append(sentiment)

"""Spliting train data into X train and Y train 
X_train will be reviws and y_train will be sentiments
"""

X_train=np.array(train_reviews)
print(X_train.shape)
y_train=np.array(train_sentiments)
print(y_train.shape)

"""**vocabulary size (total words)**

The vocabulary size (total words) must be specified. The size of the vocabulary defines the hashing space from which words are hashed and normally it has more words than the actual size.
"""

all_words = []
for review in X_train:
    for word in review:
        all_words.append(word)
different_words = set(all_words)
vocab_length=len(different_words)+100
vocab_length = {x:i for i,x in enumerate(different_words)}
vocab_length=len(different_words)
print(len(different_words))

"""We are using the text_to_word_sequence() function to split the document into words and then use a set to represent only the unique words in the document. The size of this set can be used to estimate the size of the vocabulary for one document. Then we will one hot encode them."""

from keras.preprocessing.text import text_to_word_sequence
X_train_encoded=[]
for review in X_train:
    encoded_review=[]
    result = [one_hot(word, vocab_length) for word in review]
    for x in result:
        for y in x:
            encoded_review.append(y)
    X_train_encoded.append(encoded_review)
encoded_review=np.array(encoded_review)
print(encoded_review.shape)
X_train_encoded=np.array(X_train_encoded)
print(X_train_encoded.shape)

"""The encoded document is then printed as an array of integer encoded words."""

print(X_train_encoded[0])
print(X_train_encoded[1])

"""**Pad Sequence**

Normally our model expects that each sequence each training review will be of the same length(same number of words/tokens). We can control this using the maxlen parameter.
"""

#def zero_pad_reviews(X_train_padded):
X_train_padded = pad_sequences(X_train_encoded, padding="post",maxlen=500)
print(X_train_padded[0].shape)
print(X_train_padded[1].shape)
print(X_train_padded.shape)
print(X_train_padded)
#return X_train_padded

from keras import datasets, layers, models,losses
from keras.optimizers import Adam
embedding_dim =50
max_len = 500
model = Sequential()
model.add(layers.Embedding(input_dim=vocab_length,output_dim=embedding_dim,input_length=max_len))
model.add(layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))
model.add(layers.MaxPooling1D(pool_size=2))
model.add(layers.Dropout(0.2))
model.add(layers.Flatten())
model.add(layers.Dense(256, activation='relu'))
model.add(layers.Dense(2, activation='sigmoid'))
model.summary()

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train_padded,y_train,batch_size=32, epochs=10, validation_split = 0.2)

save_model = model.save("./models/20867979_NLP_model.h5")

